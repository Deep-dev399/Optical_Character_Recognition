{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8020c9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here Sir I am Importing necessary packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.models import model_from_json\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.layers import BatchNormalization, Conv2D, AveragePooling2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import ZeroPadding2D, Activation, Dense, Flatten, Input, add\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c5b000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I am Loading and processing the A-Z dataset\n",
    "def load_az_data(path):\n",
    "    df = pd.read_csv(path, header=None)\n",
    "    labels = df[0].values\n",
    "    data = df.drop([0], axis=1).values\n",
    "    data = data.reshape((-1, 28, 28))\n",
    "    labels = labels.astype(np.uint8)\n",
    "    data = data.astype(np.float32)\n",
    "    return (data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f45fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I am Loadinging and process the MNIST 0-9 dataset\n",
    "def load_mnist_data():\n",
    "    (trainData, trainLabels), (testData, testLabels) = mnist.load_data()\n",
    "    data = np.vstack([trainData, testData])\n",
    "    labels = np.hstack([trainLabels, testLabels])\n",
    "    return (data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fbe632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I am Combining A-Z and MNIST datasets\n",
    "def combine_sets(az_path):\n",
    "    az_data, az_labels = load_az_data(az_path)\n",
    "    mnist_data, mnist_labels = load_mnist_data()\n",
    "\n",
    "    az_labels = az_labels + 10\n",
    "    data = np.vstack([az_data, mnist_data])\n",
    "    labels = np.hstack([az_labels, mnist_labels])\n",
    "\n",
    "    data = [cv2.resize(img, (32, 32)) for img in data]\n",
    "    data = np.array(data, dtype=np.float32)\n",
    "\n",
    "    data = np.expand_dims(data, axis=-1)\n",
    "    data = data / 255.0\n",
    "\n",
    "    return (data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13476311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resnet Implementation\n",
    "from tensorflow.keras.layers import BatchNormalization, Conv2D, AveragePooling2D, Activation, Dense, Flatten, Input, add\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "class ResNet:\n",
    "    @staticmethod\n",
    "    def residual_module(data, K, stride, chanDim, red=False, reg=0.0001, bnEps=2e-5, bnMom=0.9):\n",
    "        shortcut = data\n",
    "\n",
    "        bn1 = BatchNormalization(axis=chanDim, epsilon=bnEps, momentum=bnMom)(data)\n",
    "        act1 = Activation(\"relu\")(bn1)\n",
    "        conv1 = Conv2D(int(K * 0.25), (1, 1), use_bias=False, kernel_regularizer=l2(reg))(act1)\n",
    "\n",
    "        bn2 = BatchNormalization(axis=chanDim, epsilon=bnEps, momentum=bnMom)(conv1)\n",
    "        act2 = Activation(\"relu\")(bn2)\n",
    "        conv2 = Conv2D(int(K * 0.25), (3, 3), strides=stride, padding=\"same\", use_bias=False, kernel_regularizer=l2(reg))(act2)\n",
    "\n",
    "        bn3 = BatchNormalization(axis=chanDim, epsilon=bnEps, momentum=bnMom)(conv2)\n",
    "        act3 = Activation(\"relu\")(bn3)\n",
    "        conv3 = Conv2D(K, (1, 1), use_bias=False, kernel_regularizer=l2(reg))(act3)\n",
    "\n",
    "        if red:\n",
    "            shortcut = Conv2D(K, (1, 1), strides=stride, use_bias=False, kernel_regularizer=l2(reg))(act1)\n",
    "\n",
    "        x = add([conv3, shortcut])\n",
    "\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def build(width, height, depth, classes, stages, filters, reg=0.0001, bnEps=2e-5, bnMom=0.9):\n",
    "        inputShape = (height, width, depth)\n",
    "        chanDim = -1\n",
    "\n",
    "        if K.image_data_format() == \"channels_first\":\n",
    "            inputShape = (depth, height, width)\n",
    "            chanDim = 1\n",
    "\n",
    "        inputs = Input(shape=inputShape)\n",
    "        x = BatchNormalization(axis=chanDim, epsilon=bnEps, momentum=bnMom)(inputs)\n",
    "        x = Conv2D(filters[0], (3, 3), use_bias=False, padding=\"same\", kernel_regularizer=l2(reg))(x)\n",
    "\n",
    "        for i in range(0, len(stages)):\n",
    "            stride = (1, 1) if i == 0 else (2, 2)\n",
    "            x = ResNet.residual_module(x, filters[i + 1], stride, chanDim, red=True, bnEps=bnEps, bnMom=bnMom)\n",
    "\n",
    "            for j in range(0, stages[i] - 1):\n",
    "                x = ResNet.residual_module(x, filters[i + 1], (1, 1), chanDim, bnEps=bnEps, bnMom=bnMom)\n",
    "\n",
    "        x = BatchNormalization(axis=chanDim, epsilon=bnEps, momentum=bnMom)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = AveragePooling2D((8, 8))(x)\n",
    "\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(classes, kernel_regularizer=l2(reg))(x)\n",
    "        x = Activation(\"softmax\")(x)\n",
    "\n",
    "        model = Model(inputs, x, name=\"resnet\")\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7d9c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sir the file is stored at this loaction(Only for alphabets) for digits I used mnsit dataset, no need to download as above function will do.\n",
    "\n",
    "path = \"C:\\\\Users\\\\DELL\\\\OneDrive\\\\New folder\\\\A_Z Handwritten Data.csv\"\n",
    "\n",
    "\n",
    "data, labels = combine_sets(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f91a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I Binarize the labels\n",
    "le = LabelBinarizer()\n",
    "labels = le.fit_transform(labels)\n",
    "classTotals = labels.sum(axis=0)\n",
    "classWeight = {}\n",
    "for i in range(0, len(classTotals)):\n",
    "    classWeight[i] = classTotals.max() / classTotals[i]\n",
    "\n",
    "(trainX, testX, trainY, testY) = train_test_split(data, labels, test_size=0.20, stratify=labels, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fbeb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define data augmentation\n",
    "aug = ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    zoom_range=0.05,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.15,\n",
    "    horizontal_flip=False,\n",
    "    fill_mode=\"nearest\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe5491d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "# Initialize and compile the model\n",
    "EPOCHS = 10\n",
    "INIT_LR = 1e-1\n",
    "BS = 128\n",
    "# Learning rate schedule function\n",
    "def lr_schedule(epoch, lr):\n",
    "    return lr * 0.9  # Adjust this decay factor as needed\n",
    "\n",
    "# Initialize the optimizer without the decay argument\n",
    "opt = SGD(learning_rate=INIT_LR)\n",
    "\n",
    "# Compile the model\n",
    "model = ResNet.build(32, 32, 1, len(le.classes_), (3, 3, 3), \n",
    "                     (64, 64, 128, 256), reg=0.0005)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "\n",
    "# Define the learning rate scheduler callback\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "# Train the model with the learning rate scheduler\n",
    "H = model.fit(\n",
    "    aug.flow(trainX, trainY, batch_size=BS),\n",
    "    validation_data=(testX, testY),\n",
    "    steps_per_epoch=len(trainX) // BS,\n",
    "    epochs=EPOCHS,\n",
    "    class_weight=classWeight,\n",
    "    callbacks=[lr_scheduler],\n",
    "    verbose=1,\n",
    "      \n",
    ")\n",
    "\n",
    "# Save model architecture and weights\n",
    "model.save(\"handwriting_recognition_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d43a19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "labelNames = \"0123456789\"\n",
    "labelNames += \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "labelNames = [l for l in labelNames]\n",
    "\n",
    "predictions = model.predict(testX, batch_size=BS)\n",
    "print(classification_report(testY.argmax(axis=1), predictions.argmax(axis=1), target_names=labelNames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dce699",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(H.history[\"loss\"], label = \"Train Loss\")\n",
    "plt.plot(H.history[\"val_loss\"], label = \"Validation Loss\")\n",
    "\n",
    "plt.title(\"Train Loss vs Validation Loss\", size = 16)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66b3971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training loss and accuracy\n",
    "plt.plot(H.history[\"loss\"], label=\"Train Loss\")\n",
    "plt.plot(H.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "plt.title(\"Train Loss vs Validation Loss\", size=16)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868b2c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to an HDF5 file\n",
    "model.save(\"handwritten_recognition_model.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
